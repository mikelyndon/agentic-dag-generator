{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dotenv\n",
    "%pip install pydantic\n",
    "%pip install claudette\n",
    "%pip install -U gradio\n",
    "%pip install anthropic\n",
    "%pip install toolslm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from claudette import *\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type, get_type_hints\n",
    "import inspect\n",
    "import gradio as gr\n",
    "import yaml\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "from anthropic.types import TextBlock, ToolUseBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toolslm\n",
    "from toolslm.funccall import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to add your ANTHROPIC_API_KEY to a .env file to use the anthropic tools and claudette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[1] # Using claude-3-5-sonnet-20240620 by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 functions in the cell below convert the json schema to yaml. I remove any unnecessary attributes to keep things clean and reduce token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_value(obj, path):\n",
    "    \"\"\"\n",
    "    Access nested dictionary values based on a dot-separated path.\n",
    "    \"\"\"\n",
    "    for key in path.split('.'):\n",
    "        if obj is None:\n",
    "            break\n",
    "        if isinstance(obj, dict):\n",
    "            obj = obj.get(key)\n",
    "        elif isinstance(obj, list) and key.isdigit():\n",
    "            obj = obj[int(key)]\n",
    "        else:\n",
    "            return None\n",
    "    return obj\n",
    "\n",
    "def simplify_structure(obj, key_map):\n",
    "    \"\"\"\n",
    "    Simplifies the structure of nested dictionaries and lists based on a mapping.\n",
    "    Supports nested attributes using dot-separated paths.\n",
    "    \"\"\"\n",
    "    if key_map is None:\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        return {new_key: simplify_structure(get_nested_value(obj, old_key), sub_map) \n",
    "                for old_key, new_key, sub_map in key_map if get_nested_value(obj, old_key) is not None}\n",
    "    elif isinstance(obj, list):\n",
    "        return [simplify_structure(item, key_map) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def convert_json_to_yaml(json_obj):\n",
    "    mapping = [\n",
    "        ('id', 'nodeType', None),\n",
    "        ('type', 'type', None),\n",
    "        ('name', 'name', None),\n",
    "        ('metadata.description', 'description', None),\n",
    "        ('inputs', 'inputs', [\n",
    "            ('name', 'name', None),\n",
    "            ('type', 'type', None),\n",
    "            ('metadata.defaultValue', 'defaultValue', None),\n",
    "        ]),\n",
    "        ('outputs', 'outputs', [\n",
    "            ('name', 'name', None),\n",
    "            ('type', 'type', None),\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    # Apply the mapping to the provided JSON object\n",
    "    simplified_json = simplify_structure(json_obj, mapping)\n",
    "    # Convert the simplified structure to YAML\n",
    "    yaml_data = yaml.dump(simplified_json, sort_keys=False, default_flow_style=False, allow_unicode=True)\n",
    "    return yaml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using function calls to extract structured data, claude is pretty \n",
    "# good at producing valid yaml and placing it between xml tags of our choosing.\n",
    "def extract_xml_content(mixed_string, tag_name):\n",
    "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
    "    matches = re.findall(pattern, mixed_string, re.DOTALL)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yaml_list_from_json_nodes(nodes):\n",
    "  yaml_nodes = []\n",
    "  for id in nodes:\n",
    "    node = [item for item in march_nodes if item['id'] == id]\n",
    "    if len(node) == 0:\n",
    "      continue\n",
    "    node = node[0]\n",
    "    yaml_nodes.append(convert_json_to_yaml(node))\n",
    "  return '\\n'.join([f'- {item}' for item in yaml_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import existing workflows. The files in the projects are a subset of those availalble in Magpai.\n",
    "def import_workflows():\n",
    "    workflows_folder = 'workflows/yaml'\n",
    "    workflows = []\n",
    "    for filename in os.listdir(workflows_folder):\n",
    "        if filename.endswith('.yaml'):\n",
    "            with open(os.path.join(workflows_folder, filename), 'r') as f:\n",
    "                # workflow = yaml.safe_load(f)\n",
    "                workflow = {\"name\": filename[:-5], \"yaml\": f.read() }\n",
    "                workflows.append(workflow)\n",
    "    return workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_ids(workflow_names):\n",
    "  node_ids = []\n",
    "  for wf_name in workflow_names:\n",
    "    workflow_dict = [wf for wf in workflow_dicts if wf['name'] == wf_name][0]\n",
    "    ids = [node['nodeType'] for node in workflow_dict['nodes']]\n",
    "    node_ids.extend(ids)\n",
    "  node_ids = list(set(node_ids))\n",
    "  return node_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magpai Functions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "handpicked_nodes = [\n",
    "  # Image\n",
    "  'fal:sdxl',\n",
    "  'fal:sdxl-lightning',\n",
    "  # Image|Analyze\n",
    "  'replicate:yolox',\n",
    "  'replicate:blip_2',\n",
    "  # Image|Color\n",
    "  'image:colorthief',\n",
    "  # Image|Create\n",
    "  'default:image:noise',\n",
    "  'default:image:vignette',\n",
    "  'default:image:blank',\n",
    "  'image:gradient:linear',\n",
    "  'image:gradient:radial',\n",
    "  'image:draw:shape:rectangle',\n",
    "  'image:draw:shape:ellipse',\n",
    "  'image:draw:shape:circle',\n",
    "  # Image|Filter\n",
    "  'default:image:blur',\n",
    "  'default:image:drop_shadow',\n",
    "  'default:image:outline',\n",
    "  'default:image:filter',\n",
    "  # Image|Filter|Color\n",
    "  'default:image:brightness_contrast',\n",
    "  # Image|Process\n",
    "  'replicate:real_esrgan',\n",
    "  'fal:rembg',\n",
    "  # Image|Text\n",
    "  'default:image:text',\n",
    "  # Image|Utility\n",
    "  'default:image:composite',\n",
    "  'default:image:place',\n",
    "  'default:image:resize',\n",
    "  # Image|Utility|Mask\n",
    "  'default:image:make_mask',\n",
    "  'default:image:mask',\n",
    "  # Outputs\n",
    "  'default:output:image',\n",
    "  # Text|Utility\n",
    "  'default:string:format',\n",
    "  # User Inputs\n",
    "  'default:input:image',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import available nodes from Magpai. This repo includes a subset of the ~300 nodes. \n",
    "with open('data/catalogue.json') as file:\n",
    "  march_nodes = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clarify Intent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflows = import_workflows()\n",
    "workflow_dicts = [yaml.safe_load(workflow['yaml']) for workflow in workflows]\n",
    "workflow_descriptions = [{'name': workflow['name'], 'description': workflow['description']} for workflow in workflow_dicts]\n",
    "all_nodegraph_descriptions = yaml.dump(workflow_descriptions, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"You are a node-graph building assistant. You help the user build node graphs similar to Blender, Houdini or Nuke. Given a user query, a list of potential elements, and a list of available nodes, generate a graph of nodes and edges that creates an output that satisfies the users goal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_goal_sp = \"\"\"<reference_nodegraphs>\n",
    "{nodegraph_descriptions}\n",
    "</reference_nodegraphs>\n",
    "\n",
    "{base}\n",
    "\n",
    "Your current and only task is to help a user clarify their overall goal and the potential steps required to achieve that goal. The steps represent a node or sequence of nodes that make up a node graph. If the user asks for other information or attempts to build a graph, gently guide them back to identifying the goal and potential steps that satisfy the goal.\n",
    "\n",
    "You can use the <reference_nodegraphs> to help inform which steps might be relevant to the users goals and the questions you ask to clarify the overall goal and steps. Do not mention the <reference_nodegraphs> in your questions.\n",
    "\n",
    "Keep track of the overall goal and steps you have identified as well as any modifications requested by the user.\n",
    "\n",
    "If you are not able to discern the goal or steps, ask the user to clarify! Do not attempt to guess wildly.\n",
    "\n",
    "Ask for confirmation from the user that the overall goal and steps have been identified. Only call the relevant tool when the user confirms that they are satisfied with the overall goal and steps that have been identified.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Description(BaseModel):\n",
    "    \"\"\"An overall goal, key steps and summary that describes a workflow or node-graph\"\"\"\n",
    "    goal: str = Field(..., description=\"1 sentence describing the overall goal of the workflow or node-graph.\")\n",
    "    keySteps: list[str] = Field(..., description=\"The key steps required to achieve the overall goal.\")\n",
    "    summary: str = Field(..., description=\"A summary of the steps required to achieve the overall goal.\")\n",
    "\n",
    "def extract_description(\n",
    "        goal: str, # 1 sentence describing the overall goal of the workflow or node-graph\n",
    "        keySteps: list[str], # The key steps required to achieve the overall goal\n",
    "        summary: str # A summary of the steps required to achieve the overall goal\n",
    "        ) -> dict:\n",
    "    \"Extract the overall goal, key steps and summary that describes a workflow or node-graph.\"\n",
    "    return Description(goal=goal, keySteps=keySteps, summary=summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceGraphs(BaseModel):\n",
    "  names: list[str] = Field(..., description=\"The names of the reference graphs\")\n",
    "\n",
    "def extract_reference_graphs(\n",
    "    names: list[str] # Arry of names of 3 reference graphs\n",
    "    ) -> list[str]:\n",
    "    \"Extract the names of the reference graphs.\"\n",
    "    return ReferenceGraphs(names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_graphs_sp = \"\"\"\n",
    "<reference_graphs>\n",
    "{all_nodegraph_descriptions}\n",
    "</reference_graphs>\n",
    "Based on the message history, identify the 3 most relevant node graph descriptions. Return the names of the 3 most relevant node graph descriptions.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph_prompt = \"\"\"\n",
    "{base}\n",
    "\n",
    "<NodeSchemaSpecification>\n",
    "  The **Node Schema** serves as a comprehensive catalog of all node types within the system, including their attributes, expected input and output types, and optional default values for inputs. This specification is foundational for defining the capabilities and interactions of different node types.\n",
    "\n",
    "  <code language=\"yaml\">\n",
    "    nodeTypes:\n",
    "    - nodeType: \"NodeTypeIdentifier\"\n",
    "      attributes:\n",
    "        name: \"FunctionName\"\n",
    "        type: \"FunctionType\"\n",
    "      inputs:\n",
    "        - id: \"InputID\"\n",
    "          type: \"InputType\"\n",
    "          default: OptionalDefaultValue\n",
    "      outputs:\n",
    "        - id: \"OutputID\"\n",
    "          type: \"OutputType\"\n",
    "  </code>\n",
    "  <Attributes>\n",
    "  - **nodeType**: A unique identifier for the node type.\n",
    "  - **attributes**: A dictionary of attributes such as `name` and `type` to describe the node's functionality.\n",
    "  - **inputs** and **outputs**: Lists detailing the inputs and outputs for the node, including their types and, for inputs, optional default values.\n",
    "  </Attributes>\n",
    "</NodeSchemaSpecification>\n",
    "<GraphRepresentationSpecification>\n",
    "  The **Graph Representation** focuses on illustrating the interconnectedness of nodes within a graph. Each node is referenced by a unique identifier and its type, as defined in the Node Schema. This abstraction allows for a high-level view of the graph's structure and data flow.\n",
    "\n",
    "  <code language=\"yaml\">\n",
    "    nodes:\n",
    "      - id: \"UniqueNodeIdentifier\"\n",
    "        nodeType: \"NodeTypeIdentifier\"\n",
    "        inputs:\n",
    "          - name: \"InputName\"\n",
    "            value: InputValue\n",
    "\n",
    "    edges:\n",
    "      - source: \"SourceNodeIdentifier.OutputID\"\n",
    "        target: \"TargetNodeIdentifier.InputID\"\n",
    "  </code>\n",
    "\n",
    "  <Components>\n",
    "  - **nodes**: A list where each entry specifies a node in the graph, identified by a unique `id` and a `nodeType` that corresponds to an entry in the Node Schema.\n",
    "  - **edges**: Defined by linking the outputs of one or more nodes to the inputs of one or more other nodes.\n",
    "  </Components>\n",
    "</GraphRepresentationSpecification>\n",
    "<NodeSchema>\n",
    "  <code language=\"yaml\">\n",
    "    nodeTypes:\n",
    "    {yaml_nodes}\n",
    "  </code>\n",
    "</NodeSchema>\n",
    "\n",
    "<reference_nodegraphs>\n",
    "{reference_nodegraphs}\n",
    "</reference_nodegraphs>\n",
    "\n",
    "<Goal>{goal_description}</Goal>\n",
    "\n",
    "<Instructions>\n",
    "Identify the elements from the <Goal>. Start with each element and progressively work forward in generating the graph network. For each new node added to the graph:\n",
    "1. Start each step with an H2 header with the step number - eg. `## Step 1`\n",
    "2. For each step, use an H3 header, `### Thought`, to capture information about which node will be used and the purpose fo that node in building the graph.\n",
    "3. After the `### Thought` header use an H3 header, `### Act`, to capture the update to the graph. This contains the node id and nodeType in yaml format. If relevant, include the edge source and target in yaml format.\n",
    "4. After the `### Act` header use an H3 header, `### Observe`, to describe the current state of the graph.\n",
    "5. Once the graph network has been completed, wrap the final output in `<FinalGraph>` tags.\n",
    "</Instructions>\n",
    "\n",
    "<Guidelines>\n",
    "- You can use the <reference_nodegraphs> to help inform what to do at each step.\n",
    "- Favour image generation nodes over input nodes.\n",
    "- Always provide a prompt for the image generation nodes.\n",
    "- Ensure that the text nodes have the correct text content.\n",
    "- Ensure that midground and foreground elements have transparent backgrounds by using a remove background node after an image generation node.\n",
    "- A target attribute can only be connected to a maximum of one source attribute.\n",
    "</Guidelines>\n",
    "\n",
    "Here is an example of the expected output:\n",
    "\n",
    "<example>\n",
    "Certainly! I'll create a graph based on the request, including the elements that have been identified. Let's start building the graph step by step.\n",
    "## Step 1\n",
    "### Thought\n",
    "The fal:sdxl node will be used to generate a background image. The prompt will be, 'An image of a green field that is out of focus. Heavily blurred'.\n",
    "### Act\n",
    "```yaml\n",
    "nodes:\n",
    "  - id: \"Background\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"An image of a green field that is out of focus. Heavily blurred\"\n",
    "```\n",
    "### Observe\n",
    "The current graph includes a single node that generates a background image.\n",
    "\n",
    "## Step 2\n",
    "### Thought\n",
    "The default:image:composite node will be used to layer an image of poppies over the background image. The background image will be connected to the under input attribute.\n",
    "### Act\n",
    "```yaml\n",
    "nodes:\n",
    "  - id: \"Background\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"An image of a green field that is out of focus. Heavily blurred\"\n",
    "  - id: \"Composite1\"\n",
    "    nodeType: \"default:image:composite\"\n",
    "edges:\n",
    "  - source: \"Background.image\"\n",
    "    target: \"Composite1.under\"\n",
    "```\n",
    "### Observe\n",
    "The graph currently has a background image connected to a composite node. The composite node requires another image to be connected to the over input attribute.\n",
    "\n",
    "## Step 3\n",
    "### Thought\n",
    "The fal:sdxl node will be used to generate an image of poppies. The prompt will be, 'A bunch of poppies'.\n",
    "### Act\n",
    "```yaml\n",
    "nodes:\n",
    "  - id: \"Background\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"An image of a green field that is out of focus. Heavily blurred\"\n",
    "  - id: \"Composite1\"\n",
    "    nodeType: \"default:image:composite\"\n",
    "  - id: \"Poppies\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"A bunch of poppies\"\n",
    "edges:\n",
    "  - source: \"Background.image\"\n",
    "    target: \"Composite1.under\"\n",
    "  - source: \"Poppies.image\"\n",
    "    target: \"Composite1.over\"\n",
    "```\n",
    "### Observe\n",
    "The current graph composites an image of poppies over a blurred background.\n",
    "\n",
    "## Step 4\n",
    "### Thought\n",
    "The default:output:image node will be used to export the final result of the graph. This node will always be at the end of the node tree.\n",
    "### Act\n",
    "```yaml\n",
    "nodes:\n",
    "  - id: \"Background\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"An image of a green field that is out of focus. Heavily blurred\"\n",
    "  - id: \"Composite1\"\n",
    "    nodeType: \"default:image:composite\"\n",
    "  - id: \"Poppies\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"A bunch of poppies\"\n",
    "  - id: \"Output1\"\n",
    "    nodeType: \"default:output:image\"\n",
    "edges:\n",
    "  - source: \"Background.image\"\n",
    "      target: \"Composite1.under\"\n",
    "  - source: \"Poppies.image\"\n",
    "      target: \"Composite1.over\"\n",
    "  - source: \"Composite1.image\"\n",
    "      target: \"Output1.image\"\n",
    "```\n",
    "### Observe\n",
    "The graph exports an image of a bunch of poppies composited over a blurred background.\n",
    "\n",
    "## Final Graph\n",
    "Here is the final graph:\n",
    "<FinalGraph>\n",
    "```yaml\n",
    "nodes:\n",
    "  - id: \"Background\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"An image of a green field that is out of focus. Heavily blurred\"\n",
    "  - id: \"Composite1\"\n",
    "    nodeType: \"default:image:composite\"\n",
    "  - id: \"Poppies\"\n",
    "    nodeType: \"fal:sdxl\"\n",
    "    inputs:\n",
    "      - name: \"prompt\"\n",
    "        value: \"A bunch of poppies\"\n",
    "  - id: \"Output1\"\n",
    "    nodeType: \"default:output:image\"\n",
    "edges:\n",
    "  - source: \"Background.image\"\n",
    "    target: \"Composite1.under\"\n",
    "  - source: \"Poppies.image\"\n",
    "    target: \"Composite1.over\"\n",
    "  - source: \"Composite1.image\"\n",
    "    target: \"Output1.image\"\n",
    "```\n",
    "</FinalGraph>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_graph_prompt = \"\"\"\n",
    "{base}\n",
    "\n",
    "<NodeSchemaSpecification>\n",
    "  The **Node Schema** serves as a comprehensive catalog of all node types within the system, including their attributes, expected input and output types, and optional default values for inputs. This specification is foundational for defining the capabilities and interactions of different node types.\n",
    "\n",
    "  <code language=\"yaml\">\n",
    "    nodeTypes:\n",
    "    - nodeType: \"NodeTypeIdentifier\"\n",
    "      attributes:\n",
    "        name: \"FunctionName\"\n",
    "        type: \"FunctionType\"\n",
    "      inputs:\n",
    "        - id: \"InputID\"\n",
    "          type: \"InputType\"\n",
    "          default: OptionalDefaultValue\n",
    "      outputs:\n",
    "        - id: \"OutputID\"\n",
    "          type: \"OutputType\"\n",
    "  </code>\n",
    "  <Attributes>\n",
    "  - **nodeType**: A unique identifier for the node type.\n",
    "  - **attributes**: A dictionary of attributes such as `name` and `type` to describe the node's functionality.\n",
    "  - **inputs** and **outputs**: Lists detailing the inputs and outputs for the node, including their types and, for inputs, optional default values.\n",
    "  </Attributes>\n",
    "</NodeSchemaSpecification>\n",
    "<GraphRepresentationSpecification>\n",
    "  The **Graph Representation** focuses on illustrating the interconnectedness of nodes within a graph. Each node is referenced by a unique identifier and its type, as defined in the Node Schema. This abstraction allows for a high-level view of the graph's structure and data flow.\n",
    "\n",
    "  <code language=\"yaml\">\n",
    "    nodes:\n",
    "      - id: \"UniqueNodeIdentifier\"\n",
    "        nodeType: \"NodeTypeIdentifier\"\n",
    "        inputs:\n",
    "          - name: \"InputName\"\n",
    "            value: InputValue\n",
    "\n",
    "    edges:\n",
    "      - source: \"SourceNodeIdentifier.OutputID\"\n",
    "        target: \"TargetNodeIdentifier.InputID\"\n",
    "  </code>\n",
    "\n",
    "  <Components>\n",
    "  - **nodes**: A list where each entry specifies a node in the graph, identified by a unique `id` and a `nodeType` that corresponds to an entry in the Node Schema.\n",
    "  - **edges**: Defined by linking the outputs of one or more nodes to the inputs of one or more other nodes, or by specifying values directly to inputs.\n",
    "  </Components>\n",
    "</GraphRepresentationSpecification>\n",
    "<NodeSchema>\n",
    "  <code language=\"yaml\">\n",
    "    nodeTypes:\n",
    "    {yaml_nodes}\n",
    "  </code>\n",
    "</NodeSchema>\n",
    "\n",
    "<Goal>{goal_description}</Goal>\n",
    "\n",
    "<GraphToValidate>\n",
    "{graph}\n",
    "</GraphToValidate>\n",
    "\n",
    "<Instructions>\n",
    "Validate the generated graph by ensuring that all elements are included and correctly connected. Start from the final output node and work backwards to ensure that all elements are present and correctly linked. If any issues are found, make the necessary corrections and re-validate the graph.\n",
    "\n",
    "1. Wrap your analysis in opening and closing `<thinking>` tags.\n",
    "2. After you have identified any issues, generate an updated graph network wrapped in `<FinalGraph>` tags.\n",
    "</Instructions>\n",
    "\n",
    "<Guidelines>\n",
    "- Favour image generation nodes over input nodes.\n",
    "- Always provide a prompt for the image generation nodes.\n",
    "- Ensure that the text nodes have the correct text content.\n",
    "- Ensure that midground and foreground elements have transparent backgrounds by using a remove background node after an image generation node.\n",
    "- A target attribute can only be connected to a maximum of one source attribute.\n",
    "</Guidelines>\n",
    "\n",
    "Here is an example of the expected output:\n",
    "\n",
    "<example>\n",
    "Let's make sure the graph is valid. I'll check the graph based on the goal and schema specification. Let's validate the graph step by step.\n",
    "<thinking>\n",
    "It appears the graph has some issues:\n",
    "- issue1\n",
    "- issue2\n",
    "</thinking>\n",
    "Here's an updated version of the graph:\n",
    "<FinalGraph>\n",
    "```yaml\n",
    "nodes:\n",
    "- id: \"Background\"\n",
    "  nodeType: \"fal:sdxl\"\n",
    "  inputs:\n",
    "    - name: \"prompt\"\n",
    "      value: \"An image of a green field that is out of focus. Heavily blurred\"\n",
    "- id: \"Composite1\"\n",
    "  nodeType: \"default:image:composite\"\n",
    "- id: \"Poppies\"\n",
    "  nodeType: \"fal:sdxl\"\n",
    "  inputs:\n",
    "    - name: \"prompt\"\n",
    "      value: \"A bunch of poppies\"\n",
    "- id: \"Output1\"\n",
    "  nodeType: \"default:output:image\"\n",
    "edges:\n",
    "  - source: \"Background.image\"\n",
    "    target: \"Composite1.under\"\n",
    "  - source: \"Poppies.image\"\n",
    "    target: \"Composite1.over\"\n",
    "  - source: \"Composite1.image\"\n",
    "    target: \"Output1.image\"\n",
    "```\n",
    "</FinalGraph>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "  elements: list[str]\n",
    "  user_query: str\n",
    "  yaml_nodes: str\n",
    "  graph: str\n",
    "  ref_graphs: list[str]\n",
    "  description: Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAgent():\n",
    "  def __init__(self,\n",
    "               clarify_goal_sp: str,\n",
    "               reference_graphs_sp: str,\n",
    "               validate_graph_sp: str,\n",
    "               generate_graph_sp: str):\n",
    "    self.clarify_goal_sp = clarify_goal_sp\n",
    "    self.reference_graphs_sp = reference_graphs_sp\n",
    "    self.validate_graph_sp = validate_graph_sp\n",
    "    self.generate_graph_sp = generate_graph_sp\n",
    "    self.fast_model = models[2]\n",
    "    self.model = model\n",
    "\n",
    "    self.goal_chat = Chat(self.model, \n",
    "                          sp=self.clarify_goal_sp.format(base=base, nodegraph_descriptions=\"\"), tools=[extract_description])\n",
    "\n",
    "    self.ref_chat = Chat(self.fast_model,\n",
    "                         sp=self.reference_graphs_sp.format(all_nodegraph_descriptions=all_nodegraph_descriptions),\n",
    "                         tools=[extract_reference_graphs], \n",
    "                         tool_choice=\"extract_reference_graphs\")\n",
    "\n",
    "    self.graph_gen_llm = Chat(model)\n",
    "\n",
    "    self.validate_graph_llm = Chat(model)\n",
    "\n",
    "    self.state = State(elements=[], \n",
    "                       user_query=\"\", \n",
    "                       yaml_nodes=\"\", \n",
    "                       graph=\"\",\n",
    "                       ref_graphs=[],\n",
    "                       description=Description(\n",
    "                         goal=\"\",\n",
    "                         keySteps=[],\n",
    "                         summary=\"\"\n",
    "                       ))\n",
    "\n",
    "  def _generate_yaml_list(self):\n",
    "    ref_ids = get_node_ids(self.state.ref_graphs)\n",
    "    combined_ids = list(set(handpicked_nodes + ref_ids)) \n",
    "    yaml_list = create_yaml_list_from_json_nodes(combined_ids)\n",
    "\n",
    "    self.state.yaml_nodes = yaml_list\n",
    "  \n",
    "  def _generate_graph(self):\n",
    "    self.graph_gen_llm.h = [] # Clear the chat history\n",
    "\n",
    "    # Create a string of the identified reference graphs \n",
    "    reference_nodegraphs = yaml.dump([wf for wf in workflow_dicts if wf['name'] in self.state.ref_graphs], sort_keys=False)\n",
    "\n",
    "    goal_description = yaml.dump(self.state.description.model_dump(), sort_keys=False)\n",
    "\n",
    "    self._generate_yaml_list()\n",
    "\n",
    "    r = self.graph_gen_llm(\n",
    "      self.generate_graph_sp.format(\n",
    "        base=base,\n",
    "        yaml_nodes=self.state.yaml_nodes,\n",
    "        reference_nodegraphs=reference_nodegraphs,\n",
    "        goal_description=goal_description),\n",
    "        stream=True)\n",
    "    yield from r\n",
    "\n",
    "    content = [blk.text for blk in self.graph_gen_llm.c.result.content if isinstance(blk, TextBlock)][0]\n",
    "    if content:\n",
    "      xml_content = extract_xml_content(content, 'FinalGraph')\n",
    "      if len(xml_content) > 0:\n",
    "        final_graph_content = xml_content[0].strip()\n",
    "        # Remove ```yaml from beginning and ``` from end\n",
    "        final_graph_content = final_graph_content[8:-3]\n",
    "      else:\n",
    "        # TODO: Should probably return an error\n",
    "        final_graph_content = \"\"\n",
    "      self.state.graph = final_graph_content\n",
    "\n",
    "  def _validate_graph(self):\n",
    "    self.validate_graph_llm.h = [] # Clear the chat history\n",
    "\n",
    "    goal_description = yaml.dump(self.state.description.model_dump(), sort_keys=False)\n",
    "\n",
    "    self._generate_yaml_list()\n",
    "\n",
    "    r = self.validate_graph_llm(\n",
    "      self.validate_graph_sp.format(\n",
    "        base=base,\n",
    "        yaml_nodes=self.state.yaml_nodes,\n",
    "        goal_description=goal_description,\n",
    "        graph=self.state.graph),\n",
    "        stream=True)\n",
    "    yield from r\n",
    "\n",
    "    content = [blk.text for blk in self.validate_graph_llm.c.result.content if isinstance(blk, TextBlock)][0]\n",
    "    if content:\n",
    "      xml_content = extract_xml_content(content, 'FinalGraph')\n",
    "      if len(xml_content) > 0:\n",
    "        final_graph_content = xml_content[0].strip()\n",
    "        # Remove ```yaml from beginning and ``` from end\n",
    "        final_graph_content = final_graph_content[8:-3]\n",
    "      else:\n",
    "        final_graph_content = \"\"\n",
    "      self.state.graph = final_graph_content\n",
    "\n",
    "\n",
    "  def __call__(self, message ):\n",
    "    # Set the ref chat history to the goal chat history if there's an existing coversation\n",
    "    if len(self.goal_chat.h) > 0:\n",
    "      history = self.goal_chat.h\n",
    "      if history[-1]['role'] == 'assistant':\n",
    "        history = history[:-1]\n",
    "      self.ref_chat.h = history\n",
    "\n",
    "      # Identify reference graphs based on the current chat history\n",
    "      self.ref_chat()\n",
    "      for ref_blk in self.ref_chat.c.result.content:\n",
    "        if isinstance(ref_blk, ToolUseBlock):\n",
    "          self.state.ref_graphs = ref_blk.input['names']\n",
    "\n",
    "    # Create a string of the identified reference graph descriptions \n",
    "    nodegraph_descriptions = yaml.dump([wf for wf in workflow_descriptions if wf['name'] in self.state.ref_graphs], sort_keys=False)\n",
    "\n",
    "    # Update the goal system prompt to include the ref graph descriptions\n",
    "    self.goal_chat.sp = clarify_goal_sp.format(base=base, nodegraph_descriptions=nodegraph_descriptions)\n",
    "\n",
    "    goal_response = self.goal_chat(message, stream=True)\n",
    "    partial_message = \"\"\n",
    "    for o in goal_response:\n",
    "      partial_message += o\n",
    "      yield partial_message\n",
    "\n",
    "    for goal_blk in self.goal_chat.c.result.content:\n",
    "      if isinstance(goal_blk, ToolUseBlock):\n",
    "        self.state.description = Description(**goal_blk.input)\n",
    "        partial_message += f\"\\n\\nWe're ready to proceed with the following description:\\nGoal:\\n{self.state.description.goal}\\nKey Steps:\\n- \" + \"\\n- \".join(self.state.description.keySteps) + f\"\\nSummary:\\n{self.state.description.summary}\\n\"\n",
    "        yield partial_message\n",
    "\n",
    "        graph_response = self._generate_graph()\n",
    "        for o in graph_response:\n",
    "          partial_message += o\n",
    "          yield partial_message\n",
    "        partial_message += \"\\n\\n\"\n",
    "        yield partial_message \n",
    "\n",
    "        validate_response = self._validate_graph()\n",
    "        for o in validate_response:\n",
    "          partial_message += o\n",
    "          yield partial_message\n",
    "        partial_message += \"\\n\\n\"\n",
    "        yield partial_message \n",
    "        \n",
    "        partial_message += f\"Here's the final graph:\\n```yaml\\n{self.state.graph}\\n```\"\n",
    "        yield partial_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_agent = GraphAgent(\n",
    "  clarify_goal_sp=clarify_goal_sp,\n",
    "  reference_graphs_sp=reference_graphs_sp,\n",
    "  validate_graph_sp=validate_graph_prompt,\n",
    "  generate_graph_sp=generate_graph_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_theme = gr.themes.Soft(\n",
    "    primary_hue=\"slate\",\n",
    "    secondary_hue=\"sky\",\n",
    "    text_size=\"lg\",\n",
    "    spacing_size=\"lg\",\n",
    "    font=[gr.themes.GoogleFont('Solway'), 'ui-sans-serif', 'system-ui', 'sans-serif'],\n",
    ").set(\n",
    "    body_background_fill='*primary_200',\n",
    "    shadow_drop='*shadow_spread'\n",
    ")\n",
    "\n",
    "with gr.Blocks(theme=demo_theme) as demo:\n",
    "  with gr.Row():\n",
    "    with gr.Column(scale=2):\n",
    "      chatbot = gr.Chatbot(show_label=False,\n",
    "                           label=\"Chatbot\",\n",
    "                           height=\"63vh\")\n",
    "      with gr.Group():\n",
    "        with gr.Row():\n",
    "          msg = gr.Textbox(container=False,\n",
    "                           show_label=False,\n",
    "                           label=\"chat_box\",\n",
    "                           placeholder=\"Type a message...\",\n",
    "                           scale=7)\n",
    "          submit = gr.Button(\"Submit\",\n",
    "                             variant=\"primary\",\n",
    "                             scale=1)\n",
    "      clear = gr.Button(\"Clear\")\n",
    "    with gr.Column(variant=\"panel\", scale=1):\n",
    "      textbox = gr.Textbox(label=\"Info\", interactive=False)\n",
    "  \n",
    "    # If you want to print out debug info after each run, add it to the formatted_summary. Currently the reference graphs are being printed out\n",
    "    def update_infobox(history):\n",
    "      if len(graph_agent.state.ref_graphs) > 0:\n",
    "        ref_summaries = [(ref_graph['name'], ref_graph['description']['summary']) for ref_graph in workflow_dicts if ref_graph['name'] in graph_agent.state.ref_graphs]\n",
    "        formatted_summary = \"## Reference graphs:\\n\" + \"\\n\".join([f\"{name}: {summary}\\n\" for name, summary in ref_summaries])\n",
    "        return formatted_summary\n",
    "\n",
    "      return \"\"\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def msg_handler(history):\n",
    "      message = history[-1][0]\n",
    "      response = graph_agent(message)\n",
    "      history[-1][1] = \"\"\n",
    "      for o in response:\n",
    "        history[-1][1] = o\n",
    "        yield history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        msg_handler, chatbot, chatbot\n",
    "    ).then(update_infobox, chatbot, textbox)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7869\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import workflows\n",
    "def import_workflows():\n",
    "    workflows_folder = 'workflows/yaml'\n",
    "    workflows = []\n",
    "    for filename in os.listdir(workflows_folder):\n",
    "        if filename.endswith('.yaml'):\n",
    "            with open(os.path.join(workflows_folder, filename), 'r') as f:\n",
    "                # workflow = yaml.safe_load(f)\n",
    "                workflow = {\"name\": filename[:-5], \"yaml\": f.read() }\n",
    "                workflows.append(workflow)\n",
    "    return workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflows = import_workflows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_prompt = \"\"\"\n",
    "You are a helpful node graph analyser. Given a node graph and a list of node schemas you describe the overall goal of the node graph and how that goal is achieved.\n",
    "\n",
    "<NodeSchemaSpecification>\n",
    "  The **Node Schema** serves as a comprehensive catalog of all node types within the system, including their attributes, expected input and output types, and optional default values for inputs. This specification is foundational for defining the capabilities and interactions of different node types.\n",
    "\n",
    "  <code language=\"yaml\">\n",
    "    nodeTypes:\n",
    "    - nodeType: \"NodeTypeIdentifier\"\n",
    "      attributes:\n",
    "        name: \"FunctionName\"\n",
    "        type: \"FunctionType\"\n",
    "      inputs:\n",
    "        - id: \"InputID\"\n",
    "          type: \"InputType\"\n",
    "          default: OptionalDefaultValue\n",
    "      outputs:\n",
    "        - id: \"OutputID\"\n",
    "          type: \"OutputType\"\n",
    "  </code>\n",
    "  <Attributes>\n",
    "  - **nodeType**: A unique identifier for the node type.\n",
    "  - **attributes**: A dictionary of attributes such as `name` and `type` to describe the node's functionality.\n",
    "  - **inputs** and **outputs**: Lists detailing the inputs and outputs for the node, including their types and, for inputs, optional default values.\n",
    "  </Attributes>\n",
    "</NodeSchemaSpecification>\n",
    "<GraphRepresentationSpecification>\n",
    "  The **Graph Representation** focuses on illustrating the interconnectedness of nodes within a graph. Each node is referenced by a unique identifier and its type, as defined in the Node Schema. This abstraction allows for a high-level view of the graph's structure and data flow.\n",
    "\n",
    "  <code language=\"yaml\">\n",
    "    nodes:\n",
    "      - id: \"UniqueNodeIdentifier\"\n",
    "        nodeType: \"NodeTypeIdentifier\"\n",
    "      inputs:\n",
    "        - name: \"InputName\"\n",
    "          value: InputValue\n",
    "\n",
    "    edges:\n",
    "      - source: \"SourceNodeIdentifier.OutputID\"\n",
    "        target: \"TargetNodeIdentifier.InputID\"\n",
    "  </code>\n",
    "\n",
    "  <Components>\n",
    "  - **nodes**: A list where each entry specifies a node in the graph, identified by a unique `id` and a `nodeType` that corresponds to an entry in the Node Schema.\n",
    "  - **edges**: Defined by linking the outputs of one or more nodes to the inputs of one or more other nodes, or by specifying values directly to inputs.\n",
    "  </Components>\n",
    "</GraphRepresentationSpecification>\n",
    "<NodeSchema>\n",
    "  <code language=\"yaml\">\n",
    "    nodeTypes:\n",
    "    {yaml_nodes}\n",
    "  </code>\n",
    "</NodeSchema>\n",
    "<analysis_instructions>\n",
    "1. The structure of the response should be yaml as follows:\n",
    "  - 1 sentence describing the overall goal\n",
    "  - 3-8 bullet points describing the key steps\n",
    "  - 1-2 sentence summary\n",
    "2. Wrap the analysis in opening and closing `<analysis>` tags.\n",
    "</analysis_instructions>\n",
    "<example>\n",
    "  <assistant_response>\n",
    "    <analysis>\n",
    "    description:\n",
    "      - goal: Create a visually striking image featuring a random news headline with a complementary background.\n",
    "      - keySteps:\n",
    "        - Fetch and select a random news headline and image\n",
    "        - Process the image and extract its dominant color\n",
    "        - Shorten the headline using AI\n",
    "        - Generate a gradient overlay based on the image's color\n",
    "        - Compose the final image by layering the processed elements\n",
    "        - Output the result as a JPG image\n",
    "      - summary: This workflow combines data fetching, image processing, and AI-driven text manipulation to transform a news article into an eye-catching visual representation. It effectively balances automation with design principles to create a shareable news graphic.\n",
    "    </analysis>\n",
    "  </assistant_response>\n",
    "</example>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_workflows(workflows):\n",
    "    for workflow in workflows:\n",
    "        workflow_dict = yaml.safe_load(workflow['yaml'])\n",
    "        workflow_nodes = [node['nodeType'] for node in workflow_dict['nodes']]\n",
    "        workflow_yaml_list = create_yaml_list_from_json_nodes(workflow_nodes)\n",
    "        chat = Chat(model, sp=analyse_prompt.format(yaml_nodes=workflow_yaml_list))\n",
    "        r = chat(f\"Please analyse the following workflow:\\n## {workflow['name']}\\n {workflow['yaml']}\")\n",
    "        analysis = extract_xml_content(r.content[0].text, 'analysis')[0]\n",
    "\n",
    "        # Open the yaml file and prepend the name and analyis to the yaml\n",
    "        with open(f'workflows/yaml/{workflow[\"name\"]}.yaml', 'w') as f:\n",
    "            f.write('name: ' + workflow['name'] + '\\n' + analysis + '\\n' + workflow['yaml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_workflows(workflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<analysis>\n",
       "description:\n",
       "  - goal: Generate a visually appealing YouTube thumbnail featuring flying pigs based on a user-provided video description.\n",
       "\n",
       "  - keySteps:\n",
       "    - Accept user input for the video description\n",
       "    - Generate multiple image ideas using GPT-4 based on the description\n",
       "    - Select a specific image idea from the generated list\n",
       "    - Create a high-quality image using DALL-E 3 based on the selected idea\n",
       "    - Process the generated image by resizing, adding an outline, and applying a drop shadow\n",
       "    - Compose the final thumbnail by combining the processed image with additional elements\n",
       "\n",
       "  - summary: This workflow automates the creation of an eye-catching YouTube thumbnail by leveraging AI-generated content and image processing techniques. It combines user input, text generation, image creation, and various image manipulation steps to produce a professional-looking thumbnail tailored to the video's content.\n",
       "</analysis>\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: msg_01CQU7qbCbZLr2GiCGYh5Ltw\n",
       "- content: [{'text': \"<analysis>\\ndescription:\\n  - goal: Generate a visually appealing YouTube thumbnail featuring flying pigs based on a user-provided video description.\\n\\n  - keySteps:\\n    - Accept user input for the video description\\n    - Generate multiple image ideas using GPT-4 based on the description\\n    - Select a specific image idea from the generated list\\n    - Create a high-quality image using DALL-E 3 based on the selected idea\\n    - Process the generated image by resizing, adding an outline, and applying a drop shadow\\n    - Compose the final thumbnail by combining the processed image with additional elements\\n\\n  - summary: This workflow automates the creation of an eye-catching YouTube thumbnail by leveraging AI-generated content and image processing techniques. It combines user input, text generation, image creation, and various image manipulation steps to produce a professional-looking thumbnail tailored to the video's content.\\n</analysis>\", 'type': 'text'}]\n",
       "- model: claude-3-5-sonnet-20240620\n",
       "- role: assistant\n",
       "- stop_reason: end_turn\n",
       "- stop_sequence: None\n",
       "- type: message\n",
       "- usage: {'input_tokens': 3270, 'output_tokens': 203}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Message(id='msg_01CQU7qbCbZLr2GiCGYh5Ltw', content=[TextBlock(text=\"<analysis>\\ndescription:\\n  - goal: Generate a visually appealing YouTube thumbnail featuring flying pigs based on a user-provided video description.\\n\\n  - keySteps:\\n    - Accept user input for the video description\\n    - Generate multiple image ideas using GPT-4 based on the description\\n    - Select a specific image idea from the generated list\\n    - Create a high-quality image using DALL-E 3 based on the selected idea\\n    - Process the generated image by resizing, adding an outline, and applying a drop shadow\\n    - Compose the final thumbnail by combining the processed image with additional elements\\n\\n  - summary: This workflow automates the creation of an eye-catching YouTube thumbnail by leveraging AI-generated content and image processing techniques. It combines user input, text generation, image creation, and various image manipulation steps to produce a professional-looking thumbnail tailored to the video's content.\\n</analysis>\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 3270; Out: 203; Total: 3473)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model, sp=analyse_prompt.format(yaml_nodes=workflow_yaml_list))\n",
    "r = chat(f\"Please analyse the following workflow:\\n## {workflow['name']}\\n {workflow['yaml']}\")\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
